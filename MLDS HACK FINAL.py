#!/usr/bin/env python
# coding: utf-8

# In[5]:


# Import required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
url = "/Users/adishsundar/Desktop/MLDS/Amazon_reviews_plus_LLM.csv"
data = pd.read_csv(url)


# In[8]:


# 1422 rows with null data, getting rid of them can improve the accuracy of our results
new_data = data
new_data.dropna(inplace=True)


# In[9]:


new_data.head()


# In[10]:


df = new_data
df[['num1', 'num2']] = df['helpful'].str.replace('[', '').str.replace(']', '').str.split(',', expand=True).astype(int)


# In[11]:


count = 0
for index, row in new_data.iterrows():
    if (int(row['num1'])) <= ((int(row['num1']) + int(row['num2'])) * 0.3):
        count += 1


# In[12]:


print(count/len(data))


# In[14]:


from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report


# In[15]:


X_train, X_test, y_train, y_test = train_test_split(new_data['reviewText'], new_data['llm'], random_state=42)

# Create a TfidfVectorizer object to convert the text data into numerical features
vectorizer = TfidfVectorizer(stop_words='english')

# Fit the vectorizer on the training data
X_train_vec = vectorizer.fit_transform(X_train)

# Create a logistic regression classifier and fit it on the training data
clf = LogisticRegression(max_iter=10000)
clf.fit(X_train_vec, y_train)

# Use the classifier to make predictions on the test data
X_test_vec = vectorizer.transform(X_test)
y_pred = clf.predict(X_test_vec)

# Evaluate the performance of the classifier
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)

report = classification_report(y_test, y_pred)
print('Classification Report:', report)


# In[17]:


# The average reviewes per reviewer overall 
count_per_reviewer = new_data.groupby('reviewerID')
newest = count_per_reviewer.count().sort_values(by='asin', ascending=False)
newest['asin'].mean()


# In[20]:


# The average reviewes per reviewer where ChatGPT was used to generate a fake review
True_GPT = new_data[new_data['llm'] == True]
gpt = True_GPT.groupby('reviewerID').count().sort_values(by='asin', ascending=False)
gpt['asin'].mean()


# In[18]:


count = 0 

count_2 = 0

count_3 = 0

count_gpt = 0

total_count = 0

for index, row in new_data.iterrows():
    if "nail" in row["reviewerName"].lower():
        count += 1
        
    if "nail" in row["reviewerName"].lower() and "nail" in row["reviewText"].lower():
        count_2 += 1
        
    if "nail" in row["reviewerName"].lower() and "nail" in row["reviewText"].lower() and "nail" in row["summary"].lower():
        count_3 += 1
        
    if "nail" in row["reviewerName"].lower() and row['llm'] == True:
        count_gpt += 1
        
    total_count += 1
   
print(count)
print(count_2)
print(count_3)
print(count_gpt)
print(total_count)


# In[19]:


# roughly 30-40% of our results show that product names are coorelated to fake names. (We tested with various products)
96 / 291


# In[21]:


# Percent of rows that are flagged True for ChatGPT, roughly 3%
len(True_GPT) / len(new_data)


# In[22]:


# average number of characters for reviews that were not guaranteed to be generated by ChatGPT

count = 0

for index, row in new_data.iterrows():
    if row['llm'] == False:
        count += len(row['reviewText'])
        
print(count/len(new_data))


# In[23]:


# average number of characters for reviews that were generated by ChatGPT

count = 0

for index, row in True_GPT.iterrows():
    count += len(row['reviewText'])
        
print(count/len(True_GPT))


# In[26]:


from sentence_transformers import SentenceTransformer, util
from sklearn.neighbors import LSHForest
import numpy as np

# Load pre-trained model
model = SentenceTransformer('paraphrase-distilroberta-base-v2')

# Define function to compute similarity using LSH
def compute_similarity(texts, threshold=0.9):
    # Encode all sentences into embeddings
    embeddings = model.encode(texts, convert_to_tensor=True)

    # Compute similarity using LSH
    lshf = LSHForest(n_estimators=50, random_state=42)
    lshf.fit(embeddings.detach().cpu().numpy())
    neighbors = lshf.kneighbors_graph(mode='distance', n_neighbors=10)
    neighbors[neighbors < threshold] = 0

    # Return similarity matrix
    return neighbors.toarray().tolist()

# Example usage

similarity_matrix = compute_similarity(new_data['reviewText'], threshold=0.2)
print(similarity_matrix)


# In[ ]:


# Import necessary libraries
import pandas as pd
import numpy as np
import re
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Preprocess the text data
def preprocess_text(text):
    text = re.sub('[^a-zA-Z]', ' ', text) # Remove non-alphabetic characters
    text = text.lower() # Convert text to lowercase
    text = nltk.word_tokenize(text) # Tokenize text into words
    text = [word for word in text if word not in nltk.corpus.stopwords.words('english')] # Remove stop words
    text = ' '.join(text) # Join the words into a string
    return text

new_data['clean_text'] = new_data['reviewText'].apply(preprocess_text)

# Extract features using TF-IDF
tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(new_data['clean_text'])

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, new_data['llm'], test_size=0.2, random_state=42)

# Train a Naive Bayes classifier
nb_classifier = MultinomialNB()
nb_classifier.fit(X_train, y_train)

# Evaluate the performance of the model
y_pred = nb_classifier.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred))
print("Recall:", recall_score(y_test, y_pred))
print("F1-Score:", f1_score(y_test, y_pred))

# Use the model to predict whether a review is real or fake
new_review = new_data['clean_text']
new_review_tfidf = tfidf_vectorizer.transform(new_review)
print(nb_classifier.predict(new_review_tfidf))


# In[ ]:




